---
title: "Final Project: Walmart Data Exploration"
author: "Stephen Kappel (spk2131), Mayank Misra (mm3557), Mandeep Singh (ms4826)"
date: "Due: December 15, 2015"
output: pdf_document
---

```{r global_options, echo=FALSE}
knitr::opts_chunk$set(fig.height=3, warning=FALSE, message=FALSE)
```

## Introduction

For our project, we chose to use sales data provided by Walmart as part of the *Walmart Recruiting - Store Sales Forecasting* competition on Kaggle in 2014. (See https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting.) The dataset contains weekly sales for 45 Walmart stores from 2/5/2010 to 11/1/2012. The sales are broken out by store and by department. Other attributes provide further context to the sales numbers:

* _Economic indicators:_ CPI, unemployment rate, fuel prices
* _Store-specific attributes:_ store size, store type, markdowns (one numeric variable for each of five types of markdowns)
* _Other:_ temperature, holiday (indicator variable)

In our analysis, sales is the outcome variable of primary interest. We aim to understand what factors impact sales. In particular, we try to answer:

1. How do economic factors -- over which Walmart has no control -- affect Walmart's sales?
2. How well can we predict sales? In predicting sales, which predictors are most significant?

## Data Preparation

Kaggle provides the data in the form of three CSVs. We use the `merge` function to combine the three files together, resulting in aggregate weekly sales by store in a single `data.frame` with all associated features. We cast several categorical variables into `factor` types and do some column renaming. Our `data.frame` with all store-level sales aggregates is named `store.level`.

```{r data_prep}
require(plyr)

get.stores <- function(){
  stores <- read.csv('../data/stores.csv')
  stores$Store <- as.factor(stores$Store)
  return(stores)
}

get.features <- function(){
  features <- read.csv('../data/features.csv')
  features$Store <- as.factor(features$Store)
  features$Date <- as.Date(features$Date)
  return(features)
}
  
get.train <- function(){
  train <- read.csv('../data/train.csv')
  train$Store <- as.factor(train$Store)
  train$Dept <- as.factor(train$Dept)
  train$Date <- as.Date(train$Date)
  return(train)
}

get.store.level <- function(){
  # construct a data.frame with detail down to the store level (no departments)
  train <- get.train()
  train <- ddply(train, c('Store', 'Date'), summarize, Sales=sum(Weekly_Sales))
  stores <- get.stores()
  features <- get.features()
  merged <- merge(train, stores, by='Store')
  merged <- merge(merged, features, by=c('Store', 'Date'))
  merged$Sales_Millions <- merged$Sales / 10e6
  merged <- rename(merged, c('Type'='Store_Type', 'Size'='Store_Size'))
  return(merged)
}

store.level <- get.store.level()
```

As we do our analysis, we often find it helpful to normalize a column within the context of a store. This allows for more meaningful comparisons. Because this is a common operation, we define a function to add a nomalized column to a given data.frame for a specified column/attribute. To start, we add a normalized sales column to the `store.level` data.frame.

```{r normalizer}
add.normalized.col <- function(df, col.name, impute=FALSE){
  # df: data.frame (usually store.level) to add normalized column to
  # col.name: the name of the column in df which should be normalized
  # impute: if TRUE, fill in missing values as 0 (the mean)
  # the normalized column is named as 'Norm_[col.name]'
  df$temp <- df[, col.name]
  store.ply <- ddply(df, c('Store'), summarize, Mean_X=mean(temp, na.rm=TRUE), 
                     SD_X=sd(temp, na.rm=TRUE))
  df <- merge(df, store.ply, by=c('Store'))
  df$Norm_X <- (df[,col.name] - df$Mean_X) / df$SD_X
  df$Mean_X <- NULL
  df$SD_X <- NULL
  df$temp <- NULL
  if(impute){
    df$Norm_X[is.na(df$Norm_X)] <- 0
  }
  df <- rename(df, c('Norm_X'=paste('Norm_', col.name, sep='')))
  return(df)
}

store.level <- add.normalized.col(store.level, 'Sales')
```

We also commonly want to get sales aggregates after grouping and/or filtering by some attribute, so we define a function for this operation.

```{r aggr}
get.aggr.sales <- function(df, aggr.dims, filter=NULL){
  # df: data.frame (usually store.level)
  # aggr.dims: a list of column names to aggregate on
  # filter: NULL or c(column name to filter, value to filter to)
  if(!is.null(filter)){
    sub.df <- subset(df, df[,filter[1]] == filter[2])
  } else{
    sub.df <- df 
  }
  grouped <- ddply(sub.df, aggr.dims, summarize, Sales=sum(Sales), 
                   Sales_Millions=sum(Sales_Millions))
  return(grouped)
}
```

Let's take a look at the normalized distribution of store-level weekly sales. If the data turns out to be normally distributed, we will apply some statistical tests for normally distributed data.

```{r sales_distribution}
require(ggplot2)
ggplot(data=store.level, aes(x=Norm_Sales)) + geom_histogram(binwidth=0.25)
```

The distribution appears roughly symmetric and normal, although there is a light long tail on the positive side.

## Impact of economic factors

In this section we explore how economic factors (unemployment, CPI, and fuel price) relate to Walmart's sales. It's not immediately obvious how we should expect Walmart's sales to vary with economic conditions. While poor economic conditions hurt most retailers, Walmart is known for low prices. Could bad economic times drive more people to shop at Walmart?

We start by creating a boxplot of sales by unemployment rate.

```{r boxplot1}
ggplot(data=store.level, aes(x=Unemployment, y=Sales_Millions, 
  group=round_any(Unemployment, 1.0))) + geom_boxplot()
```

This plot don't show much of a relationship, but we haven't done any normalization, so this probably isn't a meaningful representation of the relationship. We normalize sales and unemployment rate by store and create an updated boxplot.

```{r boxplot2}
store.level <- add.normalized.col(store.level, 'Unemployment')
ggplot(data=store.level, aes(x=Norm_Unemployment, y=Norm_Sales, 
  group=round_any(Norm_Unemployment, 0.25))) + geom_boxplot()
```

The same data in a scatter plot with LOESS (blue) and GAM (red) curves overlaid:

```{r scatter}
ggplot(data=store.level, aes(x=Norm_Unemployment, y=Norm_Sales)) + 
  geom_point(alpha = 1/5) + geom_smooth(method='loess', color='blue') +
  geom_smooth(method='gam', color='red')
```

Within the -1 to 1 normalized unemployment range, a pattern seems to have emerged. There appears to be below-average sales when there is above-average unemployment. Let's create a pair of histograms showing number of weeks by normalized sales -- one histogram for weeks with below-average unemployment and one histogram for weeks with above-average unemployment. We want to see if these distributions are noticeably different. We also want to see the shape of these distributions to see if the data is roughly normally distributed so that a t-test could be reasonably applied.

```{r facet_hist}
store.level$Unemployment_Category <- ifelse(
  store.level$Norm_Unemployment < 0, 'Below Average', 'Above Average')
ggplot(data=store.level, aes(x=Norm_Sales)) + geom_histogram(binwidth=0.25) +
  facet_wrap(~Unemployment_Category)
```

The relationship between unemployment and sales isn't too clear from this plot, but the populations do appear to be roughly normal. We now run an F-test to check if the populations have equal variances (as assumed by a two-sample t-test).

```{r f_test}
above <- subset(store.level, Unemployment_Category == 'Above Average')
below <- subset(store.level, Unemployment_Category == 'Below Average')
var.test(above$Norm_Sales, below$Norm_Sales)
```

We reject the null hypothesis that the populations have equal variance, so we proceed with Welsh t-test for populations with unequal variances. The null hypothesis of this t-test is that the mean normalized sales during weeks with above-average unemployment rates is equal to the mean normalized sales during weeks with below-average umemployment rates.

```{r Welsh_test}
t.test(above$Norm_Sales, below$Norm_Sales, var.equal=FALSE)
```

We can confidently reject the null hypothesis. Now, we drop our assumption of normality, and perform a non-parametric test to determine if sales for weeks with above-average unemployment are drawn from a different distribution than the sales for weeks with below-average unemployment. We apply the Mann-Whitney-Wilcoxon procedure below.

```{r wilcox_test}
wilcox.test(Norm_Sales ~ Unemployment_Category, data=store.level)
```

From all the results above, we can conclude that Walmart's sales are not independent from the unemployment rate; the two variables are negatively correlated.

We repeat the same procedures for CPI and fuel price. We hide the code and output to save space, but we summarize the p-values from the tests  in the table below:

```{r cpi_fuel, include=FALSE}
# CPI
store.level <- add.normalized.col(store.level, 'CPI')
store.level$CPI_Category <- ifelse(
  store.level$Norm_CPI < 0, 'Below Average', 'Above Average')
ggplot(data=store.level, aes(x=Norm_Sales)) + geom_histogram(binwidth=0.25) +
  facet_wrap(~CPI_Category)
above <- subset(store.level, CPI_Category == 'Above Average')
below <- subset(store.level, CPI_Category == 'Below Average')
var.test(above$Norm_Sales, below$Norm_Sales)
t.test(above$Norm_Sales, below$Norm_Sales, var.equal=FALSE)
wilcox.test(Norm_Sales ~ CPI_Category, data=store.level)

# Fuel Price
store.level <- add.normalized.col(store.level, 'Fuel_Price')
store.level$Fuel_Category <- ifelse(
  store.level$Norm_Fuel_Price < 0, 'Below Average', 'Above Average')
ggplot(data=store.level, aes(x=Norm_Sales)) + geom_histogram(binwidth=0.25) +
  facet_wrap(~Fuel_Category)
above <- subset(store.level, Fuel_Category == 'Above Average')
below <- subset(store.level, Fuel_Category == 'Below Average')
var.test(above$Norm_Sales, below$Norm_Sales)
t.test(above$Norm_Sales, below$Norm_Sales, var.equal=FALSE)
wilcox.test(Norm_Sales ~ Fuel_Category, data=store.level)
```

Economic Factor | F-test 95% CI | t-test 95% CI | Mann-Whitney-Wilcoxon p-value
----------------|---------------|---------------|------------------------------
Unemployment    | (1.14, 1.31)  | (-0.22, -0.12)| 2.2e-16
CPI             | (0.83, 0.95)  | (0.11, 0.21)  | 2.2e-16
Fuel price      | (0.34, 0.39)  | (-0.15, -0.04)| 3.9e-4

All three variables tell a similar story. When external economic factors are unfavorable (high unemployment, low CPI, and high fuel prices), Walmart sales are not as strong as they are during times when external economic factors are favorable.

## Sales predictions

With the goal of creating a model to predict store-level normalized sales for a given week and store, we build a set of features and then compare the performance of a few different models.

### Features

In order to make good predictions about sales, we need to consider much more than just the economic factors we've considered so far. For example, the next couple plots show that dates will also be important in our prediction; we observe seasonality and trending patterns in the time series.

```{r trend_plots}
# plot sales by week
sales.by.week <- get.aggr.sales(store.level, c('Date'))
ggplot(data=sales.by.week, aes(x=Date, y=Sales_Millions)) + geom_line()
# plot sales by week by store type
sales.by.type <- get.aggr.sales(store.level, c('Store_Type','Date'))
ggplot(data=sales.by.type, aes(x=Date, y=Sales_Millions, group=Store_Type, 
                               color=Store_Type)) + geom_line()
# by store for type C
sales.by.store <- get.aggr.sales(store.level, c('Store', 'Date'), c('Store_Type', 'C'))
ggplot(data=sales.by.store, aes(x=Date, y=Sales_Millions, group=Store, color=Store)) +
  geom_line() + guides(color=FALSE)
```

To capture some of the seasonality and trending nature of the data, we add a month-of-year factor feature and we generate "lag" features that represent what the normalized sales were 1, 2, 3, 4, 51, and 52 weeks ago. In an attempt to capture trending, we also create a feature that represents the difference in normalized sales between last week and 53 weeks ago. By adding features representing past values of the same time series, we are creating an auto-regressive model. Because we are using auto-regressive features in addition to other predictors, this can be classified as an ARX model.

```{r lag_variables}
require(lubridate)
store.level$Month <- as.factor(month(store.level$Date))

add.lag.column <- function(df, weeks){
  # df: data.frame (usually store.level) to which lag column will be added
  # weeks: an integer representing the number of weeks ago that a lag column 
  #        will be created for
  # The added lag column is named 'Lag_[weeks]_Sales'
  shifted <- df[, c('Store', 'Date', 'Norm_Sales')]
  shifted$Date <- shifted$Date + (weeks * 7)
  shifted <- rename(shifted, c('Norm_Sales'=paste('Lag_', weeks, '_Sales', sep='')))
  df <- merge(df, shifted, c('Store', 'Date'), all.x = TRUE, all.y = FALSE)
  return(df)
}

store.level <- add.lag.column(store.level, 1)
store.level <- add.lag.column(store.level, 2)
store.level <- add.lag.column(store.level, 3)
store.level <- add.lag.column(store.level, 4)
store.level <- add.lag.column(store.level, 51)
store.level <- add.lag.column(store.level, 52)
store.level <- add.lag.column(store.level, 53)
store.level$Lag_1_53_Diff <- store.level$Lag_1_Sales - store.level$Lag_53_Sales
```

Some plots of other predictor variables we'll include in our models:

```{r other_predictors}
ggplot(data=store.level, aes(x=IsHoliday, y=Norm_Sales)) + geom_boxplot()
store.level <- add.normalized.col(store.level, 'Temperature')
ggplot(data=store.level, aes(x=Norm_Temperature, y=Norm_Sales)) + geom_point(alpha=1/5)
```
```{r correlation_matrix, fig.height=4}
require(GGally)
ggpairs(store.level[,c('Norm_Sales', 'Norm_CPI', 'Norm_Unemployment', 'Norm_Temperature', 
                       'Norm_Fuel_Price')], 
        title='Pair plot for macro predictors', axisLabels='internal',
        upper = list(continuous='cor'), lower=list(continuous='points'),
        columnLabels=c('Sales', 'CPI', 'Unemployment', 'Temperature', 'Fuel Price')) 
```

Finally, we use the five markdown attributes in the data set. We normalize the values by store. These attributes have a lot of missing values; we impute missing values to be the given store's average markdown. 

```{r markdowns, fig.height=4}
for (md in 1:5){
  store.level <- add.normalized.col(store.level, paste('MarkDown', md, sep=''), impute=TRUE)
}
ggpairs(store.level[,c('Norm_Sales', 'Norm_MarkDown1', 'Norm_MarkDown2', 'Norm_MarkDown3',
                       'Norm_MarkDown4','Norm_MarkDown5')], 
        title='Pair plot for MarkDowns', axisLabels='internal',
        upper = list(continuous='cor'), lower=list(continuous='points'),
        columnLabels=c('Sales', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'))
```

### Setup

We limit the testing/training sets to data after February 11, 2011, because we cannot calculate all of our lag features for weeks before then. We divide the remain data points randomly into an 80% training set and a 20% test set.

```{r test_train}
test.train <- subset(store.level, Date >= '2011-02-11')
set.seed(123)
row.count=nrow(test.train)
train.ind <- sample(1:row.count, size=row.count*0.8)
train.set <- test.train[train.ind, ]
test.set <- test.train[-train.ind, ]
paste(nrow(train.set), 'training records;', nrow(test.set), 'testing records')
```

To allow us to make good comparisons between models, we use a consistent set of predictors for all the models we fit in the following section. We include:

* Normalized external economic factors (unemployment rate, CPI, and fuel price)
* A holiday indicator variable with a Store_Type interaction (because we saw that store type C was less impacted by the holiday season than were the other store types)
* Normalized temperature
* Month-of-year and lag features
* Five normalized markdown features

```{r formula}
sales.formula <- Norm_Sales ~ Norm_Unemployment + Norm_CPI + Norm_Fuel_Price +
  IsHoliday*Store_Type + Norm_Temperature + Lag_1_Sales + Lag_2_Sales + Lag_3_Sales +
  Lag_4_Sales + Lag_51_Sales + Lag_52_Sales + Lag_1_53_Diff + Month + Norm_MarkDown1 +
  Norm_MarkDown2 + Norm_MarkDown3 + Norm_MarkDown4 + Norm_MarkDown5
# version without interactions to be used for our decision tree model
sales.formula.no.interactions <- Norm_Sales ~ Norm_Unemployment + Norm_CPI + Norm_Fuel_Price +
  IsHoliday + Store_Type + Norm_Temperature + Lag_1_Sales + Lag_2_Sales + Lag_3_Sales +
  Lag_4_Sales + Lag_51_Sales + Lag_52_Sales + Lag_1_53_Diff + Month + Norm_MarkDown1 +
  Norm_MarkDown2 + Norm_MarkDown3 + Norm_MarkDown4 + Norm_MarkDown5
```

We define a convenience function for evaluating the model on the training and test sets:

```{r}
get.mses <- function(model){
  test.mse <- mean((predict(model, test.set) - test.set$Norm_Sales)^2)
  train.mse <- mean((predict(model, train.set) - train.set$Norm_Sales)^2)
  paste('Train MSE:', train.mse,'; Test MSE:', test.mse)
}
```

### Model comparison

To start, we try a linear regression model. With no variable selection or shrinkage, it wouldn't be surprising to see the model overfit the data.

```{r linear}
linear.model <- lm(sales.formula, data=train.set)
summary(linear.model)
```

In the summary, we see that:

* Norm_Unemployment and Norm_CPI appear somewhat significant, but Norm_Fuel_Price does not. And, the Norm_Fuel_Price coefficient is positive, even though our earlier analysis indicated this coefficient should be negative. We suspect there is some collinearity between these three predictors, so we do not want to read too much into their relative significances.
* The IsHoliday indicator variable has a very significant positive coefficient (i.e. holidays lead to more sales), and the store type C interaction has a significant negative coefficient (which aligns with our earlier observation that store type C did not exhibit the same seasonal behavior as the other store types in our time series plots).
* All of the time-based predictors appear to be very significant.
* Of the five different markdowns, only MarkDown2 and MarkDown3 appear to be strong predictors.

How does the MSE on the training set compare to the MSE on the test set?

```{r linear_mses}
get.mses(linear.model)
```

The test error is only slightly higher than the training error. This seems to indicate that we do not have a big overfitting problem, even though we have not used any subset selection or shrinkage. Below, we apply subset selection using forward and backward stepwise regression to see if/how the results change.

```{r stepwise}
forward.model <- step(object=lm(Norm_Sales ~ 1, data=train.set), scope=sales.formula, 
                      direction='forward', trace=FALSE)
summary(forward.model)
get.mses(forward.model)

backward.model <- step(object=lm(sales.formula, data=train.set), 
                       direction='backward', trace=FALSE)
summary(backward.model)
get.mses(backward.model)
```

Using AIC to determine the optimal stopping point, the forward stepwise regression and backward stepwise regression yield exactly the same result. They include all predictors in the model except for Norm_MarkDown1, Norm_MarkDown4, Norm_MarkDown5 Norm_Temperature and Norm_Fuel_Price. The train MSE increased, but we successfully decreased the test MSE. We interpret this to mean that the increase in (squared) bias we incurred by removing predictors was less than the decrease in variance that we gained.

Instead of subset selection, we now try shrinkage. We use lasso regression. 10-fold cross validation is used to find the best value of $\lambda$. Then, we use this optimal $\lambda$ to made predictions on our test set.

```{r lasso}
require(glmnet)
train.matrix <- model.matrix(object=sales.formula, data=train.set)
train.matrix <- train.matrix[,2:ncol(train.matrix)]  # remove the (intercept) column
best.lambda <- cv.glmnet(x=train.matrix, y=train.set$Norm_Sales, nfolds=10, alpha=1)$lambda.min
best.lambda
lasso.betas <- glmnet(x=train.matrix, y=train.set$Norm_Sales, alpha=1, lambda=best.lambda)$beta
lasso.betas
test.matrix <- model.matrix(object=sales.formula, data=test.set)
test.matrix <- test.matrix[,2:ncol(test.matrix)]  # remove the (intercept) column
lasso.test.preds <-  test.matrix %*% lasso.betas
lasso.train.preds <-  train.matrix %*% lasso.betas
lasso.test.mse <- mean((lasso.test.preds - test.set$Norm_Sales)^2)
lasso.train.mse <- mean((lasso.train.preds - train.set$Norm_Sales)^2)
paste('Train MSE:', lasso.train.mse,'; Test MSE:', lasso.test.mse)
```

In this case, the optimal $\lambda$ was so small that no coefficients were shrunk all the way to zero. The coefficients are of similar relative magnitude as to what we observed with ordinary linear regression, but most are closert to zero. $\lambda$ is small because $n$ is much larger than $p$. Therefore, we don't have too much overfitting and the penalty is small. Unlike subset selection, which marginally improved our test MSE, LASSO regression increased the squared bias more than it decreased the variance, thus leading to a much higher MSE than in our original linear regression model.

Next, we try KNN regression. We start by using leave-one-out cross validation on the training set to find the best value of k.

```{r knn_tuning}
require(FNN)
for(k in 1:15){
  knn.cv.model <- knn.reg(train.matrix, k=k, y=train.set$Norm_Sales)
  print(paste('With k=', k, ', MSE is ', mean(knn.cv.model$residuals^2), '.', sep=''))
}
```

Because k=4 minimized the MSE in cross validation, we use this to make predictions on our test set.

```{r knn_pred}
knn.test.preds <- knn.reg(train.matrix, k=4, y=train.set$Norm_Sales, test=test.matrix)$pred
knn.train.preds <- knn.reg(train.matrix, k=4, y=train.set$Norm_Sales, test=train.matrix)$pred
knn.test.mse <- mean((knn.test.preds - test.set$Norm_Sales)^2)
knn.train.mse <- mean((knn.train.preds - train.set$Norm_Sales)^2)
paste('Train MSE:', knn.train.mse,'; Test MSE:', knn.test.mse)
```

For predicting Walmart's weekly store sales, KNN regression outperforms the linear models. KNN is a nonparametric method which uses only local information to make its prediction. In contrast, linear models fit a model globally and are less flexible. Because the more flexible model gives better results on the test set, we believe there is notable bias in the linear model, and more predictors would need to be collected or engineered for a linear model to yield less error.

While the KNN model does give lower error, it doesn't provide nearly the same interpretability as the linear model. In practice, if we were working for Walmart, our interest would most likely be to understand the factors impacting sales so that Walmart could make decisions and take actions that would influence future sales. From that perspective, even if the linear model yields higher error on the test set than KNN, it is still more useful.

As one final alternative (locally-fit) model, we fit a decision tree. We use the same predictors as with the previous models, but without the interation terms, because, by its nature, a decision tree considers interactions without us explicitly specifying them.

```{r decision_tree}
require(rpart)
# build the tree with training data
full.tree <- rpart(sales.formula.no.interactions, data=train.set)
get.mses(full.tree)
# based on cross-validation, prune the tree back
pruned.tree <- prune(full.tree, cp=full.tree$cptable[which.min(full.tree$
                                   cptable[,'xerror']),'CP'])
pruned.tree$cptable
get.mses(pruned.tree)
```

The decision tree performs better than lasso regression but worse than linear regression, stepwise regression, and KNN.

## Appendix

The following observations did not directly fit into the analysis presented in the body of the report, but we thought it was interesting and wanted to include it.

```{r store_size_scatter}
sales.by.size <- get.aggr.sales(store.level, c('Store', 'Store_Type', 'Store_Size'))
ggplot(data=sales.by.size, aes(x=Store_Size, y=Sales_Millions, color=Store_Type)) + 
  geom_point()
```

There is what appears to be a roughly linear relationship between a store's sales and its size. We also notice in the plot above that store type is closely related to both store sales and store size.

To get a sense of how local regression differs based on parameter values, we applied loess smoothing to this scatter plot.

```{r store_size_loess}
ggplot(data=sales.by.size, aes(x=Store_Size, y=Sales_Millions)) + 
  geom_point() + 
  geom_smooth(method='loess', size=1, span=0.5, degree=1, 
              aes(color='span 0.5; degree 1'), se=F) + 
  geom_smooth(method='loess', size=1, span=0.5, degree=2, 
              aes(color='span 0.5; degree 2'), se=F) +
  geom_smooth(method='loess', size=1, span=1, degree=1, 
              aes(color='span 1.0; degree 1'), se=F) +
  geom_smooth(method='loess', size=1, span=1, degree=2, 
              aes(color='span 1.0; degree 2'), se=F) +
  scale_color_manual('', values=c('span 0.5; degree 1'='orange', 
                                  'span 0.5; degree 2'='red', 
                                  'span 1.0; degree 1'='blue', 
                                  'span 1.0; degree 2'='green'))
```

As we expect, the local regressions with larger spans (i.e. less local), are less wiggly. And, the local regressions of degree two are more flexible than the regressions of degree 1. The regression with span of 1 and degree 1 is nearly a linear regression.
